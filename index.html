<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-163300228-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-163300228-1', { "page_path": "/2021/" + location.hash.slice(1) });
  </script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>GENEA workshop 2021</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet">
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="css/iva.min.css" rel="stylesheet">

</head>

<body id="page-top">
  <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
      <span class="d-block d-lg-none">GENEA workshop 2021</span>
      <!-- <span class="d-none d-lg-block">
        <img src="img/avatar.png" class="img-fluid img-profile rounded-circle mx-auto mb-5" alt="">
        
      </span> -->
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#home">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#important-dates">Important dates</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#workshop-programme">Workshop programme</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#call-for-papers">Call for papers</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#reproducibility-award">Reproducibility Award</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#invited-speakers">Invited speakers</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#accepted-papers">Accepted papers</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#organising-committee">Organising committee</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#program-committee">Program committee</a>
        </li>
        
      </ul>
    </div>
  </nav>

  <div class="container-fluid p-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="home">
      <div class="w-100">
        <div class="row">
          <div class="col-md-9 col-sm-12">
            <h1 class="mb-0">GENEA Workshop 2021
            </h1>

            <div class="subheading mb-5">Generation and Evaluation of Non-verbal Behaviour for Embodied Agents</div>
            <!--  -->

            <p class="mb-5">
              <b>Official workshop of ICMI 2021 – October 22</b><br><br>
              The GENEA (Generation and Evaluation of Non-verbal Behaviour for Embodied Agents) Workshop
              2021 aims at bringing together researchers that use different methods for non-verbal-behaviour generation
              and evaluation, and hopes to stimulate the discussions on how to improve both the
              generation methods and
              the evaluation of the results. We invite all interested researchers to submit a paper related to their
              work in the area and to participate in the workshop. This is the second installment of the GENEA Workshop, 
              for more information about the 2020 installment, please go <a href="https://genea-workshop.github.io/2020/" target="_blank">here</a>.<br><br>

              <b>IMPORTANT</b> <br>
              The workshop is as an official workshop of <a href="https://icmi.acm.org/2021/" target="_blank">ACM
                ICMI’21</a> and will be held <b>virtually over Zoom on October 22nd</b>. For information about registration for the workshop please go to <a href="https://icmi.acm.org/2021/index.php?id=registration" target="_blank">the official ICMI registration page</a>. Please note that at least one of the paper authors needs to be registered to "cover" the submitted papers.
            </p>
            <div class="row justify-content-center">

              <img src="img/avatar.png"  class="img-fluid rounded" class="mt-2" width=300 alt="">
            </div>
          </div>
          <div class="col-md-3 d-none d-md-block">
            <a class="twitter-timeline" data-tweet-limit=3 href="https://twitter.com/WorkshopGenea?ref_src=twsrc%5Etfw" target="_blank">Follow us on twitter <i class="fab fa-twitter"></i></a>
            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
          </div>
          <div class="col-xs-12 d-md-none mt-4 text-center w-100">
            <a href="https://twitter.com/WorkshopGenea" target="_blank">Follow us on Twitter <i class="fab fa-twitter"></i></a>
          </div>
        </div>
      </div>

    </section>

    <hr class="m-0">

    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="important-dates">
      <div class="w-100">
        <h2 class="mb-5">Important dates</h2>
        <h4>Timeline for regular workshop submissions:</h4>
        <div class="row">
          <div class="col"><s>12th</s> 16th June, 2021</div>
          <div class="col">Abstract deadline</div>
        </div>
        <div class="row">
          <div class="col"><s>15th</s> 22nd June, 2021</div>
          <div class="col">Submission deadline</div>
        </div>
        <div class="row">
          <div class="col">15th July, 2021</div>
          <div class="col">Notification of acceptance</div>
        </div>
        <div class="row">
          <div class="col">1st September, 2021</div>
          <div class="col">Deadline for camera-ready papers</div>
        </div>
        <div class="row">
          <div class="col">22nd October, 2021</div>
          <div class="col">Workshop</div>
        </div>
      </div>
    </section>

    <hr class="m-0">

    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="workshop-programme">
      <div class="w-100">
        <h2 class="mb-5">Workshop programme</h2>
        <h4>Tentative workshop programme:</h4>
        <div>Note, all times are in the <a href="https://everytimezone.com/?t=6171ff00" target="_blank">EST timezone</a>. The workshop will take place virtually over Zoom on October 22nd. <br><br></div>
        <div class="row">
          <div class="col">08:00&nbsp;-&nbsp;08:10</div>
          <div class="col-10">Opening statement</div>
        </div>
        <div class="row">
          <div class="col">08:10&nbsp;-&nbsp;08:50</div>
          <div class="col-10"><a href="#hatice_gunes" class="js-scroll-trigger">Keynote 1 (Hatice Gunes)</a></div>
        </div>
        <div class="row">
          <div class="col">08:50&nbsp;-&nbsp;09:35</div>
          <div class="col-10">Paper presentations <br>
            
          </div>
        </div>
        <div class="row">
          <ul style="list-style: none">
            <li><span style="margin-right: 10px;">08:50</span> Wu et al. <a href="https://openreview.net/forum?id=ykvm7OLh7B" target="_blank">Probabilistic Human-like Gesture Synthesis from Speech using GRU-based WGAN</a></li>
            <li><span style="margin-right: 10px;">09:05</span> Schneeberger et al. <a href="https://openreview.net/forum?id=GjjPtEVdSLB" target="_blank">Influence of Movement Energy and Affect Priming on the Perception of Virtual Characters Extroversion and Mood</a></li>
            <li><span style="margin-right: 10px;">09:20</span> Lee et al. <a href="https://openreview.net/forum?id=o8CpxaBurZQ" target="_blank">Crossmodal clustered contrastive learning: Grounding of spoken language to gesture</a></li>
          </ul>
        </div>
        <div class="row">
          <div class="col">09:35&nbsp;-&nbsp;09:50</div>
          <div class="col-10">Break</div>
        </div>
        <div class="row">
          <div class="col">09:50&nbsp;-&nbsp;10:30</div>
          <div class="col-10"><a href="#louis-philippe_morency" class="js-scroll-trigger">Keynote 2 (Louis-Philippe Morency)</a></div>
        </div>
        <div class="row">
          <div class="col">10:30&nbsp;-&nbsp;10:45</div>
          <div class="col-10">Break</div>
        </div>
        <div class="row">
          <div class="col">10:45&nbsp;-&nbsp;10:50</div>
          <div class="col-10"><a href="#reproducibility-award" class="js-scroll-trigger">Reproducibility Award</a> announcement</div>
        </div>
        <div class="row">
          <div class="col">10:50&nbsp;-&nbsp;11:50</div>
          <div class="col-10">Group discussions</div>
        </div>
        <div class="row">
          <div class="col">11:50&nbsp;-&nbsp;11:55</div>
          <div class="col-10">Closing remarks</div>
        </div>
        
      </div>
    </section>

    <hr class="m-0">

    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="call-for-papers">
      <!-- justify-content-center -->
      <div class="w-100">
        <h2 class="mb-5">Call for papers</h2>

        <div class="iva-item d-flex flex-column flex-md-row justify-content-between mb-5">
          <div class="iva-content">
            <h4>Overview</h4>
            Generating nonverbal behaviours, such as gesticulation, facial expressions and gaze, is of great importance
            for natural interaction with embodied agents such as virtual agents and social robots. At present, behaviour
            generation is typically powered by rule-based systems, data-driven approaches, and their hybrids. For
            evaluation, both objective and subjective methods exist, but their application and validity are frequently a
            point of contention.<br>
            <br>

            This workshop asks “What will be the behaviour-generation methods of the future? And how can we evaluate
            these methods using meaningful objective and subjective metrics?” The aim of the workshop is to bring
            together researchers working on the generation and evaluation of nonverbal behaviours for embodied agents to
            discuss the future of this field. To kickstart these discussions, we invite all interested researchers to
            submit a paper for presentation at the workshop.<br>
            <br>

            GENEA 2021 is the second GENEA workshop and an official workshop of ACM ICMI’21, which will take place
            either in Montreal, Canada, or online. Accepted submissions will be included in the adjunct ACM ICMI
            proceedings.<br>
            <br>

            <h4>Paper topics include (but are not limited to) the following</h4>
            <ul>
              <li>Automated synthesis of facial expressions, gestures, and gaze movements</li>
              <li>Audio- and music-driven nonverbal behaviour synthesis</li>
              <li>Closed-loop nonverbal behaviour generation (from perception to action)</li>
              <li>Nonverbal behaviour synthesis in two-party and group interactions</li>
              <li>Emotion-driven and stylistic nonverbal behaviour synthesis</li>
              <li>New datasets related to nonverbal behaviour</li>
              <li>Believable nonverbal behaviour synthesis using motion-capture and 4D scan data</li>
              <li>Multi-modal nonverbal behaviour synthesis</li>
              <li>Interactive/autonomous nonverbal behavior generation</li>
              <li>Subjective and objective evaluation methods for nonverbal behaviour synthesis</li>
              <li>Guidelines for nonverbal behaviours in human-agent interaction</li>
            </ul>

            For papers specifically on the topic of healthcare, whether for generating or understanding nonverbal
            behaviours, consider submitting to the workshop on Socially-Informed AI for Healthcare, also taking place at
            ICMI’21. The website of that workshop can be found at: <a href="https://social-ai-for-healthcare.github.io"
              target="_blank">social-ai-for-healthcare.github.io</a> <br>
            <br>

            <h4>Submission guidelines</h4>
            The reviewing will be double blind, so submissions should be anonymous: do not include the authors' names, affiliations or any clearly identifiable information in the paper (including in the Acknowledgments and references). It is appropriate to cite past work of the authors if these citations are treated like any other (e.g., "Smith [5] approached this problem by....") - omit references only if it would be obviously identifying the authors. Paper chairs will desk reject non-anonymous papers after reviewing begins. 
            <br><br>
            Submitted papers should conform to the latest ACM publication format. All authors should submit manuscripts for review in a double column format to ensure adherence to page limits. Please note that a non-anonymous author block may require a larger space than the anonymized version. For LaTeX templates and examples, please click on the following link: <a href="https://www.acm.org/publications/taps/word-template-workflow" target="_blank">https://www.acm.org/publications/taps/word-template-workflow</a>, download the zip package entitled Primary Article Template - LaTeX, and use the sample-sigconf.tex template with \documentclass[sigconf,review]{acmart} to add line numbers. We suggest you to use the LaTeX templates. You will find Word templates and examples on the same webpage mentioned. Authors who do decide to use the Word template should be made aware that an extra validation step may be required during the camera-ready process. 
            <br><br>
            We will accept long (8 pages) and short (4 pages) paper submissions, along with posters (3 page papers), all
            in the double-column ACM conference format. Pages containing only references do not count toward the page
            limit for any of the paper types. Submissions should be made in PDF format through <a href="https://openreview.net/group?id=ACM.org/ICMI/2021/Workshop/GENEA" target="_blank">OpenReview</a>.
            <br><br>
            To encourage authors to make their work reproducible and reward the effort that this requires, we have introduced the <a class="js-scroll-trigger" href="#reproducibility-award">GENEA Workshop Reproducibility Award</a>.
          </div>
        </div>
      </div>
    </section>

    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="reproducibility-award">
      <!-- justify-content-center -->
      <div class="w-100">
        <h2 class="mb-5">Reproducibility Award</h2>
        Reproducibility is a cornerstone of the scientific method. Lack of reproducibility is a serious issue in contemporary research which we want to address at our workshop. To encourage authors to make their papers reproducible, and to reward the effort that reproducibility requires, we are introducing the GENEA Workshop Reproducibility Award. All short and long papers presented at the GENEA Workshop will be eligible for this award. Please note that it is the camera-ready version of the paper which will be evaluated for the reward.
        <br><br>
        The award is awarded to the paper with the greatest degree of reproducibility. The assessment criteria include:
        <ul>
          <li>ease of reproduction (ideal: just works, if there is code - it is well documented and we can run it)</li>
          <li>extent (ideal: all results can be verified)</li>
          <li>data accessibility (ideal: all data used is publicly available)</li>
        </ul>
      </div>

    </section>
    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="invited-speakers">
      <div class="w-100">
        <h2 class="mb-5">Invited speakers</h2>

        <h3 class="mt-5" id="hatice_gunes"><a href="https://www.cl.cam.ac.uk/~hg410/">Hatice Gunes (University of Cambridge)</a></h3>
        <div class="row">
          <div class="col-2"><a href="https://www.cl.cam.ac.uk/~hg410/"> <img src="https://www.cl.cam.ac.uk/~hg410/HG2019.jpg"
            style=" margin-right: 30px; margin-bottom: 10px" class="img-fluid rounded" alt="Hatice Gunes"></a></div>
          <div class="col-10">
            <h5>Biography</h5>
            Dr Hatice Gunes (Senior Member, IEEE) is a Reader with the Department of Computer Science and Technology, University of Cambridge, UK, leading the Affective Intelligence and Robotics (AFAR) Lab. Her expertise is in the areas of affective computing and social signal processing cross-fertilizing research in multimodal interaction, computer vision, signal processing, machine learning and social robotics. Dr Gunes’ team has published over 120 papers in these areas (citations > 5,700) and has received various awards and competitive grants, with funding from the Engineering and Physical Sciences Research Council UK (EPSRC), Innovate UK, British Council, and EU Horizon 2020. Dr Gunes is the former President of the Association for the Advancement of Affective Computing (2017-2019), the General Co-Chair of ACII 2019, and the Program Co-Chair of ACM/IEEE HRI 2020 and IEEE FG 2017. She was the Chair of the Steering Board of IEEE Transactions on Affective Computing in the period of 2017-2019. She has also served as an Associate Editor for IEEE Transactions on Affective Computing, IEEE Transactions on Multimedia, and Image and Vision Computing Journal. In 2019, Dr Gunes has been awarded the prestigious EPSRC Fellowship to investigate adaptive robotic emotional intelligence for wellbeing (2019-2024) and has been named a Faculty Fellow of the Alan Turing Institute – UK’s national centre for data science and artificial intelligence.
            <br>
            <br>
            <h5>Talk - Data-driven Robot Social Intelligence</h5>
            Designing artificially intelligent systems and interfaces with socio-emotional skills is a challenging task. Progress in industry and developments in academia provide us a positive outlook, however, the artificial social and emotional intelligence of the current technology is still limited. My lab’s research has been pushing the state of the art in a wide spectrum of research topics in this area, including the  design and creation of new datasets; novel feature representations and learning algorithms for sensing and understanding human nonverbal behaviours in solo, dyadic and group settings; and the data-driven generation of socially appropriate agent behaviours. In this talk, I will present some of my research team’s explorations in these areas, including audio-driven robot upper-body motion synthesis, and a dataset and a continual learning approach for assessing social appropriateness of robot actions.
        </div>
      </div>

        <h3 class="mt-5" id="louis-philippe_morency"><a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency (Carnegie Mellon University)</a></h3>
        <div class="row">
          <div class="col-2"><a href="https://www.cs.cmu.edu/~morency/"><img src="https://www.cs.cmu.edu/~morency/index_files/image001.jpg"
            style=" margin-right: 30px; margin-bottom: 10px" class="img-fluid rounded" alt="Louis-Philippe Morency"></a></div>
          <div class="col-10"><h5>Biography</h5>
            Louis-Philippe Morency is Associate Professor in the Language Technology Institute at Carnegie Mellon University where he leads the Multimodal Communication and Machine Learning Laboratory (MultiComp Lab). He was formerly research faculty in the Computer Sciences Department at University of Southern California and received his Ph.D. degree from MIT Computer Science and Artificial Intelligence Laboratory. His research focuses on building the computational foundations to enable computers with the abilities to analyze, recognize and predict subtle human communicative behaviors during social interactions. He received diverse awards including AI’s 10 to Watch by IEEE Intelligent Systems, NetExplo Award in partnership with UNESCO and 10 best paper awards at IEEE and ACM conferences. His research was covered by media outlets such as Wall Street Journal, The Economist and NPR.
            <br>
            <br>
            <h5>Talk - Multimodal AI: Learning Nonverbal Signatures</h5>
            Human face-to-face communication is a little like a dance, in that participants continuously adjust their behaviors based on verbal and nonverbal cues from the social context. Today's computers and interactive devices are still lacking many of these human-like abilities to hold fluid and natural interactions. One such capability is to generate natural-looking gestures when animating a virtual avatar or robot. In this talk, I will present some of our recent work towards learning nonverbal signatures of human speakers. Central to this research problem is the technical challenge of grounding, which links in this case spoken language with generated nonverbal gestures. This is one key step towards our longer-term vision of Multimodal AI, a family of technologies able to analyze, recognize and generate human subtle communicative behaviors in social context.
          </div>
        </div>
      </div>
    </section>

    <hr class="m-0">

    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="accepted-papers">
      <div class="w-100">
        <h2 class="mb-5">Accepted papers</h2>
      <ul>
        <li>Wu et al. <a href="https://openreview.net/forum?id=ykvm7OLh7B" target="_blank">Probabilistic Human-like Gesture Synthesis from Speech using GRU-based WGAN</a></li>
        <li>Schneeberger et al. <a href="https://openreview.net/forum?id=GjjPtEVdSLB" target="_blank">Influence of Movement Energy and Affect Priming on the Perception of Virtual Characters Extroversion and Mood</a></li>
        <li>Lee et al. <a href="https://openreview.net/forum?id=o8CpxaBurZQ" target="_blank">Crossmodal clustered contrastive learning: Grounding of spoken language to gesture</a></li>
      </ul>
    </div>
    </section>

    


    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="organising-committee">
      <div class="w-100">
        <h2 class="mb-5">Organising committee</h2>
        <p>
          The main contact address of the workshop is: <a
            href="mailto:genea-contact@googlegroups.com">genea-contact@googlegroups.com</a>. <br> <br>
          </p>
        <h4>Workshop organisers</h4>

        <div class="row">
          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="img/taras.jpg" class="img-fluid rounded" alt="Taras Kucherenko">
              </div>
              <div class="col-7">
                <a href="https://svito-zar.github.io/" target="_blank" style="font-weight: bold;">Taras Kucherenko</a>
                <br>
                KTH Royal Institute of Technology <br> Sweden

              </div>
            </div>
            <hr>
          </div>
          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="img/zerrin.jpg" class="img-fluid rounded"  alt="Zerrin Yumak">
              </div>
              <div class="col-7">
                <a href="http://www.zerrinyumak.com" target="_blank" style="font-weight: bold;">Zerrin Yumak</a>
                <br>
                Utrecht University <br> The Netherlands
              </div>
            </div>
            <hr>
          </div>

        </div>

        <div class="row">
          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="img/gustav.jpeg" class="img-fluid rounded" alt="Gustav Eje Henter">
              </div>
              <div class="col-7">

                <a href="https://people.kth.se/~ghe/" target="_blank" style="font-weight: bold;">Gustav Eje Henter</a>
                <br>
                KTH Royal Institute of Technology <br> Sweden


              </div>
            </div>
            <hr>
          </div>
          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="img/pieter.jpeg" width="100%" class="img-fluid rounded" alt="Pieter Wolfert">
              </div>
              <div class="col-7">
                  <a href="https://www.pieterwolfert.com" target="_blank" style="font-weight: bold;">Pieter Wolfert</a>
                  <br>
                  IDLab, Ghent University - imec <br> Belgium
              </div>
            </div>
            <hr>
          </div>
        </div>

        <div class="row">
          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="img/youngwoo.jpg" class="img-fluid rounded" alt="Youngwoo Yoon">
              </div>
              <div class="col-7">

                <a href="https://sites.google.com/view/youngwoo-yoon/" target="_blank" style="font-weight: bold;">Youngwoo Yoon</a>
                <br>ETRI & KAIST <br> South Korea


              </div>
            </div>
            <hr>
          </div>
          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="img/patrik.jpg" class="img-fluid rounded" alt="Patrik Jonell">
              </div>
              <div class="col-7">
                  <a href="https://www.patrikjonell.se" target="_blank" style="font-weight: bold;">Patrik Jonell</a>
                  <br>
                  KTH Royal Institute of Technology <br> Sweden
              </div>
            </div>
            <hr>
          </div>
        </div>
        
      </div>
    </section>
    <hr class="m-0">
    
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="program-committee">
      <!-- justify-content-center -->
      <div class="w-100">
        <h2 class="mb-5">Program committee</h2>
        <ul>
          <li>Chaitanya Ahuja (Carnegie Mellon University, United States of America)</li>
          <li>Sean Andrist (Microsoft Research, United States of America)</li>
          <li>Jonas Beskow (KTH Royal Institute of Technology, Sweden)</li>
          <li>Carlos Busso (University of Texas at Dallas, United States of America)</li>
          <li>Oya Celiktutan (King’s College London, United Kingdom)</li>
          <li>Ylva Ferstl (Trinity College Dublin, Ireland)</li>
          <li>Carlos Ishi (Advanced Telecommunications Research Institute International, Japan)</li>
          <li>Minsu Jang (Electronics and Telecommunications Research Institute, South Korea)</li>
          <li>James Kennedy (Disney Research, United States of America)</li>
          <li>Stefan Kopp (Bielefeld University, Germany)</li>
          <li>Zofia Malisz (KTH Royal Institute of Technology, Sweden)</li>
          <li>Rachel McDonnell (Trinity College Dublin, Ireland)</li>
          <li>Michael Neff (University of California, Davis, United States of America)</li>
          <li>Catherine Pelachaud (Sorbonne University, France)</li>
          <li>Wim Pouw (Radboud University, the Netherlands)</li>
          <li>Tiago Ribeiro (Soul Machines, New Zealand)</li>
        </ul>
      </div>

    </section>

    <hr class="m-0">


  </div>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/iva.min.js"></script>

</body>

</html>
